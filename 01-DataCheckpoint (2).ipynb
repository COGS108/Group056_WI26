{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "16b860a9f5fc21240e9d88c0ee13691518c3ce67be252e54a03b9b5b11bd3c7a"
      }
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "## Rubric\n\nInstructions: DELETE this cell before you submit via a `git push` to your repo before deadline. This cell is for your reference only and is not needed in your report. \n\nScoring: Out of 10 points\n\n- Each Developing  => -2 pts\n- Each Unsatisfactory/Missing => -4 pts\n  - until the score is \n\nIf students address the detailed feedback in a future checkpoint they will earn these points back\n\n\n|                  | Unsatisfactory                                                                                                                                                                                                    | Developing                                                                                                                                                                                              | Proficient                                     | Excellent                                                                                                                              |\n|------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|\n| Data relevance   | Did not have data relevant to their question. Or the datasets don't work together because there is no way to line them up against each other. If there are multiple datasets, most of them have this trouble | Data was only tangentially relevant to the question or a bad proxy for the question. If there are multiple datasets, some of them may be irrelevant or can't be easily combined.                       | All data sources are relevant to the question. | Multiple data sources for each aspect of the project. It's clear how the data supports the needs of the project.                         |\n| Data description | Dataset or its cleaning procedures are not described. If there are multiple datasets, most have this trouble                                                                                              | Data was not fully described. If there are multiple datasets, some of them are not fully described                                                                                                      | Data was fully described                       | The details of the data descriptions and perhaps some very basic EDA also make it clear how the data supports the needs of the project. |\n| Data wrangling   | Did not obtain data. They did not clean/tidy the data they obtained.  If there are multiple datasets, most have this trouble                                                                                 | Data was partially cleaned or tidied. Perhaps you struggled to verify that the data was clean because they did not present it well. If there are multiple datasets, some have this trouble | The data is cleaned and tidied.                | The data is spotless and they used tools to visualize the data cleanliness and you were convinced at first glance                      |\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# COGS 108 - Data Checkpoint",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Authors\n\nInstructions: REPLACE the contents of this cell with your team list and their contributions. Note that this will change over the course of the checkpoints\n\nThis is a modified [CRediT taxonomy of contributions](https://credit.niso.org). For each group member please list how they contributed to this project using these terms:\n> Analysis, Background research, Conceptualization, Data curation, Experimental investigation, Methodology, Project administration, Software, Visualization, Writing – original draft, Writing – review & editing\n\n\n- Nehal Choudhary: Data Description \n- Kaushal Jagdish: Data Description\n- Hasaam Butt: EDA + data cleaning\n- Thejo Tattala: Addressing Feedback\n- Gerardo Suarez: Updating timeline",
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Research Question",
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "source": "Can we accurately predict the presence of Alzheimer’s disease based on patients’ clinical and demographic variables (like age, education level, medical conditions, etc)? More specifically, which clinical features are most predictive of Alzheimer’s diagnosis, and how well do supervised classification models (like logistic regression, random forest, etc) perform in distinguishing between Alzheimer’s and non-Alzheimer’s cases? To answer this, we will analyze how predictive performance metrics (accuracy, AUC, precision, recall) vary across models and identify key variables that contribute most to prediction.",
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Background and Prior Work",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Alzheimer’s disease research has long relied on large, structured datasets to understand how dementia develops, how it is diagnosed, and which patient characteristics are most strongly associated with disease progression. One of the most influential efforts in this area is the Alzheimer’s Disease Neuroimaging Initiative (ADNI), which was established to systematically collect longitudinal clinical, demographic, cognitive, imaging, and biomarker data from individuals with normal cognition, mild cognitive impairment, and Alzheimer’s disease. These datasets reflect how Alzheimer’s status is investigated using a combination of patient characteristics alongside clinical indicators and health measurements. The ADNI study demonstrated that Alzheimer’s diagnosis and progression can be characterized meaningfully using a combination of standardized clinical assessments and patient-specific information, showing that predicting Alzheimer’s related outcomes from structured patient data is a well-established research direction. ¹ ² Similarly, the Open Access Series of Imaging Studies (OASIS) provides longitudinal MRI and clinical data from older adults with and without dementia, enabling comparative analysis across cognitive states2. The OASIS project showed that structured datasets containing demographic information, cognitive scores, and clinical labels are valuable for distinguishing between healthy aging and Alzheimer’s-related decline. Together, ADNI and OASIS illustrate that Alzheimer’s research commonly integrates patient demographics with clinical indicators to study diagnosis and disease progression, and they have become benchmark datasets for evaluating predictive approaches. Building on these structured datasets, prior research has shown that supervised machine learning models can successfully classify Alzheimer’s disease status using routinely collected clinical variables, even without relying on imaging data or invasive biomarkers. Suresh et al. developed a lightweight machine learning framework that compared traditional linear models such as logistic regression with non-linear models including random forests to predict Alzheimer’s disease from clinical features. Their results demonstrated that machine learning approaches can achieve strong predictive performance using accessible patient data, while also allowing for analysis of feature importance to identify which variables contribute most to diagnostic decisions. These studies commonly evaluate model performance using metrics such as accuracy, area under the receiver operating characteristic curve (AUC), precision, and recall, since each metric captures different aspects of classification error.⁴ Many studies also look at which features are most significant to explain what models use to identify the strongest predictors of diagnosis. While ADNI and OASIS are commonly used to benchmark datasets in Alzheimer's research, our project will apply a similar approach in using accessible clinical datasheets, ours being from Kaggle. This Kaggle dataset includes diagnosis labels along with demographic, lifestyle, and health variables which make it suitable for analysis. Like prior machine learning approaches to Alzheimer’s predictions, we are able to compare models such as logistic regression and random forest and can evaluate them using accuracy, AUC, precision, or recall. We can also examine which features are contributing the most to predictions, which can help identify which patient variables are most important in distinguishing Alzheimer’s cases.\nWhile prior studies demonstrate strong predictive performance using curated research datasets such as ADNI and OASIS, these datasets are often collected under highly controlled research conditions and may not fully reflect broader or more diverse populations. Additionally, imaging-based approaches may not always be accessible in real-world clinical settings. Our project contributes by evaluating how well prediction models perform using publicly available clinical datasets that emphasize demographic and lifestyle variables, and by comparing model interpretability and feature importance across data sources.\n\n\n1. Petersen RC et al. (2010). Alzheimer’s Disease Neuroimaging Initiative (ADNI): clinical characterization. Neurology, 74(3), 201–209.\nhttps://pmc.ncbi.nlm.nih.gov/articles/PMC2809036/ ^\n2. Marcus DS et al. (2010). Open access series of imaging studies: longitudinal MRI data in nondemented and demented older adults. Journal of Cognitive Neuroscience, 22(12), 2677–2684.\nhttps://pmc.ncbi.nlm.nih.gov/articles/PMC2895005/ ^\n3. Suresh V et al. (2024). A lightweight machine learning tool for Alzheimer’s disease prediction. Alzheimer’s & Dementia: Diagnosis, Assessment & Disease Monitoring.\nhttps://pmc.ncbi.nlm.nih.gov/articles/PMC12620993/ ^\n4. Vinodvadde. Alzheimer’s Clinical Dataset. Kaggle.\nhttps://www.kaggle.com/datasets/vinodvadde/alzeimers-clinical-dataset ^\n",
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Hypothesis\n",
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "source": "Age will be the strongest predictor of Alzheimer’s diagnosis. This is because Alzheimer’s risk increases substantially with age, and many prior studies show age is one of the most consistent factors associated with dementia progression. Therefore, we expect models trained on demographic and health features will rely heavily on age to separate diagnosed vs. non-diagnosed individuals.\n",
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Data",
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "source": "Our dataset has 2149 patient records and 35 variables including patient ID, diagnosis, demographics, and clinical health measures related to Alzheimer’s disease. A few key demographic variables are age (years, ranging from 60-90, which is the strongest risk factor for Alzheimer's), education level (ranging from 0-3) and BMI (measured in kg/m2 with normal values ranging from 18.5-24.9). Some cardiovascular variables include Systolic and Diastolic Blood Pressure (which are measured in mmHg, with normal BP typically being <120/<80 mmHg, respectively). \n\nThe dataset also includes metabolic variables like total cholesterol, LDL, HDL, and triglycerides (measured in mg/dL). The MMSE score variable measures cognitive function on a 0-30 scale, where scores above 25 are considered normal. The dataset also includes ADL and functional assessment scores (ranging from 0-10) to reflect a patient’s ability to perform daily tasks independently. Lastly, the dataset includes binary indicators (0=no, 1=yes) for conditions like diabetes, depression, confusion, etc.\n\nBecause our research question focuses primarily on how demographic factors relate to Alzheimer’s diagnosis, it is important that all the demographic and clinical variables are measured on the same patients. Although our hypothesis focuses on age as the strongest predictor, the clinical and metabolic variables are important because Alzheimer’s disease is influenced by both demographic and health factors. Therefore, medical features like blood pressure, cholesterol levels, and diabetes can interact with age and help us determine whether age is the dominant predictor after controlling for overall health. \n\nOne major concern with this dataset is self-selection bias. The people in this study are likely those who have the time and money to visit a doctor regularly or use health tracking apps. This means that the sample is probably skewed towards people with higher incomes and better education. Due to the fact that these individuals often have a brain which is, in a way, more resilient to damage, a model trained on them might not work very well for the general public, especially for people in lower-income areas who don’t have the same access to healthy food or medical care.\n\nAnother issue is that many variables, such as DietQuality or PhysicalActivity, are usually self-reported. This would lead to recall bias, where people, especially those already experiencing symptoms of memory loss, might not remember their habits accurately. People also tend to round up/down how much they exercise or how much they drink to look better on paper. If the data contains such inconsistencies resulting from the people not being completely honest or accurate, an ML model might end up chasing patterns that aren’t actually real, making its predictions less reliable in a real clinical setting.\n\n",
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "code",
      "source": "# Run this code every time when you're actively developing modules in .py files.  It's not needed if you aren't making modules\n#\n## this code is necessary for making sure that any modules we load are updated here \n## when their source code .py files are modified\n\n%load_ext autoreload\n%autoreload 2",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Setup code -- this only needs to be run once after cloning the repo!\n# this code downloads the data from its source to the `data/00-raw/` directory\n# if the data hasn't updated you don't need to do this again!\n\n# if you don't already have these packages (you should!) uncomment this line\n# %pip install requests tqdm\n\nimport sys\nsys.path.append('./modules') # this tells python where to look for modules to import\n\nimport get_data # this is where we get the function we need to download data\n\n# replace the urls and filenames in this list with your actual datafiles\n# yes you can use Google drive share links or whatever\n# format is a list of dictionaries; \n# each dict has keys of \n#   'url' where the resource is located\n#   'filename' for the local filename where it will be stored \ndatafiles = [\n    { 'url': 'https://www.kaggle.com/datasets/vinodvadde/alzeimers-clinical-dataset?select=alzheimers_disease_data.csv', 'filename':'alzeimers clinical dataset'}\n]\n\nget_data.get_raw(datafiles,destination_directory='data/00-raw/')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### Dataset #1 \n\nInstructions: \n1. Change the header from Dataset #1 to something more descriptive of the dataset\n2. Write a few paragraphs about this dataset. Make sure to cover\n   1. Describe the important metrics, what units they are in, and giv some sense of what they mean.  For example \"Fasting blood glucose in units of mg glucose per deciliter of blood.  Normal values for healthy individuals range from 70 to 100 mg/dL.  Values 100-125 are prediabetic and values >125mg/dL indicate diabetes. Values <70 indicate hypoglycemia. Fasting idicates the patient hasn't eaten in the last 8 hours.  If blood glucose is >250 or <50 at any time (regardless of the time of last meal) the patient's life may be in immediate danger\"\n   2. If there are any major concerns with the dataset, describe them. For example \"Dataset is composed of people who are serious enough about eating healthy that they voluntarily downloaded an app dedicated to tracking their eating patterns. This sample is likely biased because of that self-selection. These people own smartphones and may be healthier and may have more disposable income than the average person.  Those who voluntarily log conscientiously and for long amounts of time are also likely even more interested in health than those who download the app and only log a bit before getting tired of it\"\n3. Use the cell below to \n    1. load the dataset \n    2. make the dataset tidy or demonstrate that it was already tidy\n    3. demonstrate the size of the dataset\n    4. find out how much data is missing, where its missing, and if its missing at random or seems to have any systematic relationships in its missingness\n    5. find and flag any outliers or suspicious entries\n    6. clean the data or demonstrate that it was already clean.  You may choose how to deal with missingness (dropna of fillna... how='any' or 'all') and you should justify your choice in some way\n    7. You will load raw data from `data/00-raw/`, you will (optionally) write intermediate stages of your work to `data/01-interim` and you will write the final fully wrangled version of your data to `data/02-processed`\n4. Optionally you can also show some summary statistics for variables that you think are important to the project\n5. Feel free to add more cells here if that's helpful for you\n",
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndf = pd.read_csv(\"data/00-raw/alzheimers_disease_data.csv\")\ndf.head()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "df.head()\ndf.info()\ndf.columns\ndf[\"PatientID\"].nunique(), df.shape[0]",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "df.shape",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "df.isna().sum().sum()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "numeric_cols = df.select_dtypes(include=\"number\").columns\n\ncontinuous_cols = [\n    col for col in numeric_cols\n    if df[col].nunique() > 10 and col != \"PatientID\"\n]\n\noutlier_counts = {}\n\nfor col in continuous_cols:\n    q1 = df[col].quantile(0.25)\n    q3 = df[col].quantile(0.75)\n    iqr = q3 - q1\n    lower = q1 - 1.5 * iqr\n    upper = q3 + 1.5 * iqr\n\n    outlier_counts[col] = ((df[col] < lower) | (df[col] > upper)).sum()\n\npd.Series(outlier_counts).sort_values(ascending=False)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "df_clean = df.drop_duplicates()\n\nprint(df.shape[0] - df_clean.shape[0])        # number of duplicates removed\nprint(df_clean.isna().sum().sum())            # total missing after duplicates removed\nprint(df_clean[\"PatientID\"].duplicated().sum())  # duplicate PatientID count\n\ndf_clean.to_csv(\"data/01-interim/alzheimers_interim.csv\", index=False)\ndf_clean.to_csv(\"data/02-processed/alzheimers_processed.csv\", index=False)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Ethics",
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "source": "A. Data Collection\n\nA.1 Informed Consent\nThis project uses publicly available datasets from Kaggle and does not involve collecting new data from human subjects. Because we are not directly interacting with participants or gathering information ourselves, informed consent is not something we can obtain or control within the scope of this project. We assume that the original dataset publishers were responsible for ensuring appropriate consent procedures. However, we recognize that when working with health-related data, ethical data collection practices are critical, and the limits of our visibility into original consent processes represent a constraint of using secondary data.\n\nA.2 Collection Bias\nSince we are not collecting data ourselves, we cannot directly mitigate biases introduced during the original data collection process. However, we acknowledge that collection bias may exist in publicly available Alzheimer’s datasets. For example, certain demographic groups (e.g., older adults with higher education levels or individuals from specific regions) may be overrepresented. We will examine the dataset’s demographic distributions and class balance to identify potential sources of bias and discuss how these may affect model generalizability and interpretation.\n\nA.3 Limit PII Exposure\nThe datasets used in this project do not include direct personally identifiable information (PII) such as names, addresses, or contact details. Because we are working with already anonymized public datasets and are not collecting additional personal information, the risk of PII exposure is minimal. We will not attempt to re-identify individuals or combine these data with external datasets that could increase re-identification risk.\n\nA.4 Downstream Bias Mitigation\nWhere demographic attributes such as age or sex are available, we will analyze model performance across subgroups to assess whether certain populations experience higher error rates. This allows us to evaluate potential downstream bias, such as disproportionately high false negatives in specific groups. While we cannot alter the original dataset composition, we can transparently report disparities and interpret results cautiously.\n\n\nB. Data Storage\n\nB.1 Data Security\nAll datasets will be stored locally on password-protected computers and within private GitHub repositories accessible only to team members. Because the data are publicly available and anonymized, the risk associated with unauthorized access is low; however, we will still follow standard best practices, including not sharing raw data files publicly beyond their original sources.\n\nB.2 Right to Be Forgotten\nBecause we are not collecting data directly from individuals and the datasets are publicly distributed by third parties, we do not have the ability to remove specific individuals’ records. This checklist item is more relevant to organizations that directly collect and manage user-submitted data. We will not redistribute modified versions of the dataset in a way that would create additional data retention concerns.\n\nB.3 Data Retention Plan\nThis project uses static public datasets solely for course-related analysis. We do not plan to maintain long-term storage of these data beyond the duration of the course. Since the datasets are publicly hosted and not maintained by us, a formal long-term retention plan is not required. Any local copies will be deleted after project completion if no longer needed.\n\n\nC. Analysis\n\nC.1 Missing Perspectives\nBecause this project is conducted within a classroom context, we do not directly engage with patients, clinicians, or Alzheimer’s advocacy groups. However, we recognize that predictive models in healthcare can have real-world implications. We will therefore interpret results cautiously and avoid overstating conclusions, especially regarding clinical decision-making.\n\nC.2 Dataset Bias\nWe will examine the dataset for class imbalance (e.g., disproportionate numbers of Alzheimer’s vs. non-Alzheimer’s cases) and demographic skew. If imbalances are present, we will consider mitigation strategies such as resampling techniques or reporting multiple evaluation metrics. We will also acknowledge omitted variable bias, as important factors (e.g., genetic markers or socioeconomic status) may not be included.\n\nC.3 Honest Representation\nAll visualizations, summary statistics, and reported metrics will be presented transparently and without selectively highlighting favorable results. We will report multiple performance metrics (accuracy, AUC, precision, recall) rather than relying on a single measure, ensuring a balanced representation of model strengths and weaknesses.\n\nC.4 Privacy in Analysis\nWe will not display or attempt to reconstruct personally identifying information. Since the dataset does not contain direct PII, privacy risks during analysis are minimal. We will avoid producing visualizations or summaries that could inadvertently reveal small-group characteristics if subgroup sizes are extremely small.\n\nC.5 Auditability\nAll preprocessing, modeling steps, and evaluation procedures will be documented in our Jupyter notebook. Code will be version-controlled using GitHub to ensure reproducibility. This allows others to audit our methodology and replicate findings if needed.\n\n\nD. Modeling\n\nD.1 Proxy Discrimination\nSome variables, such as education level or certain health conditions, could correlate with socioeconomic status or other protected characteristics. We will examine feature importance outputs to assess whether the model is relying heavily on variables that could serve as unintended proxies. While excluding such variables may reduce predictive accuracy, we will discuss ethical trade-offs transparently.\n\nD.2 Fairness Across Groups\nWhere demographic variables are available, we will evaluate model performance across subgroups (e.g., by age or sex) to assess whether false positive or false negative rates differ substantially. Because failing to identify Alzheimer’s (false negatives) could delay care, we will pay particular attention to disparities in recall.\n\nD.3 Metric Selection\nWe recognize that optimizing for accuracy alone can mask important trade-offs. In healthcare-related classification tasks, false negatives and false positives have different implications. Therefore, we will evaluate AUC, precision, and recall in addition to accuracy to better understand model performance across multiple dimensions.\n\nD.4 Explainability\nWe will use interpretable models (logistic regression) and feature importance measures (for random forest) to explain which variables contribute most strongly to predictions. This improves transparency and reduces the risk of treating the model as a “black box.”\n\nD.5 Communicate Limitations\nWe will clearly state that our model is exploratory and not a clinical diagnostic tool. The dataset is secondary, potentially biased, and primarily cross-sectional. Any conclusions will be framed within these limitations to prevent misinterpretation.\n\n\nE. Deployment\n\nE.1 Monitoring and Evaluation\nThis model is not being deployed as a real-world system and will only be used for exploratory academic analysis. Because there is no production deployment, ongoing monitoring for concept drift or real-world performance changes is not applicable.\n\nE.2 Redress\nSince our project does not make real-world decisions and is not used in clinical settings, there is no direct mechanism through which individuals could be harmed by model outputs. Therefore, a formal redress process is not necessary within this academic context.\n\nE.3 Roll Back\nOur model is not deployed in a production environment, so there is no system to disable or roll back. This checklist item applies primarily to operational machine learning systems.\n\nE.4 Unintended Use\nAlthough the model will not be deployed, we acknowledge that predictive models in healthcare could be misused if interpreted as diagnostic tools. We will clearly communicate that our analysis is educational and exploratory, and not suitable for medical decision-making.\n",
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Team Expectations ",
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "source": "In terms of communication, we will primarily communicate on our text group chat and use Google Docs + Github for project work. Our team should usually respond to messages within a day or two. We will meet once a week as needed, most likely virtually.\nIn terms of collaboration, we will give constructive feedback respectfully and with solution-focused language. We will also assume good intent and focus on improvements rather than criticism.\n*In terms of task distribution, we will communicate our tasks and deadlines during our meetings and assign them on our Google Doc. Our team is expected to complete assigned work by the agreed deadlines. *\nIn terms of challenges, we will communicate with the team as soon as possible if we are struggling or can’t make a deadline so work can be redistributed.\n",
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Project Timeline Proposal",
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "source": "Instructions: Replace this with your timeline.  **PLEASE UPDATE your Timeline!** No battle plan survives contact with the enemy, so make sure we understand how your plans have changed.  Also if you have lost points on the previous checkpoint fix them",
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Project Timeline Proposal (Updated)\n\n| Meeting Date | Meeting Time | Completed Before Meeting | Discuss at Meeting |\n|--------------|--------------|--------------------------|-------------------|\n| 2/12 | 11 AM | Confirm Alzheimer’s clinical dataset from Kaggle; skim dataset documentation/variables; draft initial research question + hypothesis | Finalize research question/hypothesis; assign roles (data wrangling, EDA, modeling, writeup); decide repo structure for saving data outputs |\n| 2/19 | 11 AM | Add dataset CSV into repo (`data/00-raw/`); run initial checks (`head`, `info`, `shape`, unique IDs); identify missingness and obvious issues | Agree on cleaning plan (handling missing values, encoding categorical variables, checking duplicates); confirm what “clean/interim/processed” files we will save to `data/01-interim/` and `data/02-processed/` |\n| 2/26 | 11 AM | Create initial EDA (distributions, correlations, diagnosis breakdown); run simple outlier scan (IQR / unusual values); summarize dataset limitations/bias risks | Decide which features to keep vs drop; choose baseline models + evaluation metrics; discuss any dataset concerns (self-selection bias, self-reported variables) to include in checkpoint writeup |\n| 3/4 | 11 AM | Build baseline models (e.g., logistic regression + random forest); run train/test split evaluation; document performance results | Compare baseline performance; pick next steps for improvement (feature engineering, regularization/tuning, handling imbalance); finalize checkpoint narrative sections |\n| Before submission | — | Final notebook pass: ensure code runs top-to-bottom; confirm files saved into correct `data/` folders; verify reproducibility and remove unused cells | Final review + submission checklist; confirm all required sections are filled (Ethics, Team Expectations, Timeline, Data wrangling evidence) |",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}